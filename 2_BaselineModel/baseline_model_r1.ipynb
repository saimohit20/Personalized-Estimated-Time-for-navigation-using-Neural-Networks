{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc-lKxYs0pfx"
      },
      "source": [
        "# Baseline Model\n",
        "\n",
        "## Table of Contents\n",
        "1. [Load dataset](#load-dataset)\n",
        "2. [Feature Selection](#feature-selection)\n",
        "3. [Implementation](#implementation)\n",
        "4. [Evaluation](#evaluation)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "ibI_fHbUQjF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oto5BB4t0pf2"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import random\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load dataset\n",
        "Load preprocessed dataset and print basic details."
      ],
      "metadata": {
        "id": "c8G5_jmE0tPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3MWauQ3N06KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/opencampus_all_files/combined_data_r1.csv')"
      ],
      "metadata": {
        "id": "ed71t52-HExO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfZQKrVg0pf5"
      },
      "source": [
        "## 2. Feature Selection\n",
        "\n",
        "Selected features describes terrain characteristics and cumulative statistics of current track."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOIJ9xv80pf6"
      },
      "outputs": [],
      "source": [
        "# Feature selection\n",
        "# Example: Selecting only two features for a simple baseline model\n",
        "X = df[['Elevation', 'Slope_prev', 'Slope_next', 'Angle', 'Distance', 'Cumulative_Slope']]\n",
        "y = df['Speed']\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of datasets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "6PrRXSWADx06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjECCs2Y0pf7"
      },
      "source": [
        "##  3. Implementation\n",
        "In following notebook two models were implemented:\n",
        "- Linear Regression\n",
        "- Neural Network\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline model 1 - Linear Regression"
      ],
      "metadata": {
        "id": "7UMUl9g2mGQt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn9eWOn10pf8"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file\n",
        "filename = 'lr_model.joblib'\n",
        "joblib.dump(lr_model, filename)"
      ],
      "metadata": {
        "id": "2SHUojlEx6BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline model 2 - Simple Neural Network"
      ],
      "metadata": {
        "id": "-1N6WpAUmJdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=X_train.shape[1], activation='relu'),  # First hidden layer\n",
        "    Dense(32, activation='relu'),  # Second hidden layer\n",
        "    Dense(1)  # Output layer (no activation since we're doing regression)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n",
        "\n",
        "# Optionally, plot the training history (e.g., loss or MAE over epochs)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sl_RvnG1mMs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model as a `.keras` zip archive.\n",
        "model.save('r1_init_nn_model.keras')"
      ],
      "metadata": {
        "id": "CSXk8Em27VEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbKtVyLp0pf9"
      },
      "source": [
        "## 4. Evaluation\n",
        "\n",
        "Evaluation based on loss and MAE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load test file"
      ],
      "metadata": {
        "id": "j53HtqoPiaGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip test files\n",
        "!unzip /content/drive/MyDrive/opencampus_all_files/Rider1_test.zip -d /content"
      ],
      "metadata": {
        "id": "0PnKeCvNyUwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation 1 - Linear Regression"
      ],
      "metadata": {
        "id": "F79HFxv1c7HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load linear model\n",
        "lr_model = joblib.load('/content/drive/MyDrive/opencampus_all_files/models/r1_init_lr_model.joblib')"
      ],
      "metadata": {
        "id": "A8LXO0L1zxI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder containing the files\n",
        "input_folder_path = '/content/content/Rider1_test/'\n",
        "output_folder_path = '/content/content/Rider1_test_LR/'\n",
        "\n",
        "# Create output_folder_path\n",
        "if not os.path.exists(output_folder_path):\n",
        "    os.makedirs(output_folder_path)\n",
        "\n",
        "# List all CSV files in the folder\n",
        "csv_files = [f for f in os.listdir(input_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Iterate through each file in the folder\n",
        "for file in csv_files:\n",
        "    file_path = os.path.join(input_folder_path, file)\n",
        "\n",
        "    # Read test file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Feature selection\n",
        "    real_time = df['Time']\n",
        "    X = df[['Elevation', 'Slope_prev', 'Slope_next', 'Angle', 'Distance', 'Cumulative_Slope']]\n",
        "    y = df['Speed']\n",
        "\n",
        "    # Make predictions on the new data\n",
        "    y_pred = lr_model.predict(X)\n",
        "\n",
        "    # Add the predicted values as a new column 'Speed_pred' in the original DataFrame\n",
        "    df['Speed_pred'] = y_pred\n",
        "\n",
        "    # Calculate MAE between df['Speed_pred'] and df['Speed']\n",
        "    mae = mean_absolute_error(df['Speed'], df['Speed_pred'])\n",
        "    print(f\"File: {file} | Mean Absolute Error: {mae}\")\n",
        "\n",
        "    # Initialize Time column\n",
        "    df['Time_pred'] = float(df['Time'].iloc[0])\n",
        "\n",
        "    # Compute predicted time\n",
        "    for i in range(2, len(df)):\n",
        "        if df.loc[i, 'Speed_pred'] < 0:\n",
        "            df.loc[i, 'Speed_pred'] = 0\n",
        "        if pd.notna(df.loc[i, 'Speed_pred']):\n",
        "            if df.loc[i, 'Speed_pred'] != 0:\n",
        "                df.loc[i, 'Time_pred'] = (\n",
        "                    df.loc[i - 1, 'Time_pred'] +\n",
        "                    (df.loc[i, 'Distance'] - df.loc[i - 1, 'Distance']) / df.loc[i, 'Speed_pred']\n",
        "                )\n",
        "            else:\n",
        "                df.loc[i, 'Time_pred'] = df.loc[i - 1, 'Time_pred']\n",
        "\n",
        "    # Save the processed DataFrame to a new file (optional)\n",
        "    output_file_path = os.path.join(output_folder_path, f\"lr_{file}\")\n",
        "    df.to_csv(output_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "eGnbQDG2WO-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation 2 - Neural Network"
      ],
      "metadata": {
        "id": "EvfWLhhPdC6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load linear model\n",
        "nn_model = tf.keras.models.load_model('/content/drive/MyDrive/opencampus_all_files/models/r1_init_nn_model.keras')"
      ],
      "metadata": {
        "id": "Z-1-feh5c-Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder containing the files\n",
        "input_folder_path = '/content/content/Rider1_test/'\n",
        "output_folder_path = '/content/content/Rider1_test_NN/'\n",
        "\n",
        "# Create output_folder_path\n",
        "if not os.path.exists(output_folder_path):\n",
        "    os.makedirs(output_folder_path)\n",
        "\n",
        "# List all CSV files in the folder\n",
        "csv_files = [f for f in os.listdir(input_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Iterate through each file in the folder\n",
        "for file in csv_files:\n",
        "    file_path = os.path.join(input_folder_path, file)\n",
        "\n",
        "    # Read test file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Feature selection\n",
        "    real_time = df['Time']\n",
        "    X = df[['Elevation', 'Slope_prev', 'Slope_next', 'Angle', 'Distance', 'Cumulative_Slope']]\n",
        "    y = df['Speed']\n",
        "\n",
        "    # Make predictions on the new data\n",
        "    # Normalize the features using StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    X_new_scaled = scaler.transform(X)\n",
        "    y_pred = nn_model.predict(X_new_scaled)\n",
        "\n",
        "    # Add the predicted values as a new column 'Speed_pred' in the original DataFrame\n",
        "    df['Speed_pred'] = y_pred\n",
        "\n",
        "    # Calculate MAE between df['Speed_pred'] and df['Speed']\n",
        "    mae = mean_absolute_error(df['Speed'], df['Speed_pred'])\n",
        "    print(f\"File: {file} | Mean Absolute Error: {mae}\")\n",
        "\n",
        "    # Initialize Time column\n",
        "    df['Time_pred'] = float(df['Time'].iloc[0])\n",
        "\n",
        "    # Compute predicted time\n",
        "    for i in range(2, len(df)):\n",
        "        if df.loc[i, 'Speed_pred'] < 0:\n",
        "            df.loc[i, 'Speed_pred'] = 0\n",
        "        if pd.notna(df.loc[i, 'Speed_pred']):\n",
        "            if df.loc[i, 'Speed_pred'] != 0:\n",
        "                df.loc[i, 'Time_pred'] = (\n",
        "                    df.loc[i - 1, 'Time_pred'] +\n",
        "                    (df.loc[i, 'Distance'] - df.loc[i - 1, 'Distance']) / df.loc[i, 'Speed_pred']\n",
        "                )\n",
        "            else:\n",
        "                df.loc[i, 'Time_pred'] = df.loc[i - 1, 'Time_pred']\n",
        "\n",
        "    # Save the processed DataFrame to a new file (optional)\n",
        "    output_file_path = os.path.join(output_folder_path, f\"nn_{file}\")\n",
        "    df.to_csv(output_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "BO8T_aTjXKlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation summary"
      ],
      "metadata": {
        "id": "_MtweafKlcKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for processed files\n",
        "output_folder_path = '/content/content/Rider1_test_LR/'\n",
        "summary_lr = []\n",
        "processed_files = [f for f in os.listdir(output_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "for file in processed_files:\n",
        "    file_path = os.path.join(output_folder_path, file)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract the last value of 'Time' and 'Time_pred'\n",
        "    last_time = df['Time'].iloc[-1]\n",
        "    last_time_pred = df['Time_pred'].iloc[-1]\n",
        "\n",
        "    # Calculate percentage difference\n",
        "    percentage_diff = ((last_time_pred - last_time) / last_time) * 100 if last_time != 0 else None\n",
        "\n",
        "    # Append to summary\n",
        "    summary_lr.append({\n",
        "        'file': file,\n",
        "        'last_time': last_time,\n",
        "        'last_time_pred': last_time_pred,\n",
        "        'percentage_diff': percentage_diff\n",
        "    })"
      ],
      "metadata": {
        "id": "RtCMC4XYi6QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for processed files\n",
        "output_folder_path = '/content/content/Rider1_test_NN/'\n",
        "summary_nn = []\n",
        "processed_files = [f for f in os.listdir(output_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "for file in processed_files:\n",
        "    file_path = os.path.join(output_folder_path, file)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract the last value of 'Time' and 'Time_pred'\n",
        "    last_time = df['Time'].iloc[-1]\n",
        "    last_time_pred = df['Time_pred'].iloc[-1]\n",
        "\n",
        "    # Calculate percentage difference\n",
        "    percentage_diff = ((last_time_pred - last_time) / last_time) * 100 if last_time != 0 else None\n",
        "\n",
        "    # Append to summary\n",
        "    summary_nn.append({\n",
        "        'file': file,\n",
        "        'last_time': last_time,\n",
        "        'last_time_pred': last_time_pred,\n",
        "        'percentage_diff': percentage_diff\n",
        "    })"
      ],
      "metadata": {
        "id": "L_Q6xesa8UbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print summary\n",
        "summary_lr_df = pd.DataFrame(summary_lr).sort_values(by=['file'])\n",
        "print(summary_lr_df)\n",
        "print(f\"Percentage difference in LR model: {summary_lr_df['percentage_diff'].abs().mean()}\\n\")\n",
        "\n",
        "summary_nn_df = pd.DataFrame(summary_nn).sort_values(by=['file'])\n",
        "print(summary_nn_df)\n",
        "print(f\"Percentage difference in NN model: {summary_nn_df['percentage_diff'].abs().mean()}\")"
      ],
      "metadata": {
        "id": "WU5ltW-L8RfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}