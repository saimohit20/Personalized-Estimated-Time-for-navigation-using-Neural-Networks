{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLH2jBYfHy2o"
      },
      "source": [
        "# Model comparison\n",
        "\n",
        "## Table of Contents\n",
        "1. [Libraries](#libraries)\n",
        "2. [Load models](#load_models)\n",
        "3. [Load data](#load-data)\n",
        "4. [Results](#results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4yKkspkHy2p"
      },
      "source": [
        "## 1. Libraries\n",
        "Install and load necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSmQCXP5Hy2q"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load models\n",
        "Load final models of all three riders."
      ],
      "metadata": {
        "id": "VeXX-cHjOXVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "enc5tCy9E-RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Neural Network Models"
      ],
      "metadata": {
        "id": "0WWCjUQyHtvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models\n",
        "r1_model = keras.models.load_model('/content/drive/MyDrive/opencampus_all_files/models/r1_mse_nn_model.keras')\n",
        "\n",
        "# Initialize the model\n",
        "r3_model = Sequential()\n",
        "r3_model.add(Dense(128, input_dim=6, activation='relu'))\n",
        "r3_model.add(Dense(256, activation='relu'))  # Large hidden layer to capture more complex patterns\n",
        "r3_model.add(Dropout(0.2))  # Dropout to prevent overfitting\n",
        "r3_model.add(Dense(128, activation='relu'))\n",
        "r3_model.add(Dropout(0.2))  # Dropout again after each hidden layer\n",
        "r3_model.add(Dense(1))  # Regression output\n",
        "\n",
        "\n",
        "# Load the weights\n",
        "r3_model.load_weights('/content/drive/MyDrive/opencampus_all_files/models/r3_mse_nn_model.keras')\n",
        "\n",
        "# Initialize the model\n",
        "r7_model = Sequential()\n",
        "r7_model.add(Dense(128, input_dim=6, activation='relu'))\n",
        "r7_model.add(Dense(256, activation='relu'))  # Large hidden layer to capture more complex patterns\n",
        "r7_model.add(Dropout(0.2))  # Dropout to prevent overfitting\n",
        "r7_model.add(Dense(128, activation='relu'))\n",
        "r7_model.add(Dropout(0.2))  # Dropout again after each hidden layer\n",
        "r7_model.add(Dense(1))  # Regression output\n",
        "\n",
        "\n",
        "# Load the weights\n",
        "r7_model.load_weights('/content/drive/MyDrive/opencampus_all_files/models/r7_nn_model.keras')\n"
      ],
      "metadata": {
        "id": "2RIVLIqBcCgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Linear Models"
      ],
      "metadata": {
        "id": "FpL-i6agHxOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load linear models\n",
        "r1_model = joblib.load('/content/drive/MyDrive/opencampus_all_files/models/r1_init_lr_model.joblib')\n",
        "r3_model = joblib.load('/content/drive/MyDrive/opencampus_all_files/models/r3_init_lr_model.joblib')\n",
        "r7_model = joblib.load('/content/drive/MyDrive/opencampus_all_files/models/r7_lr_model.joblib')"
      ],
      "metadata": {
        "id": "A2cCQTBCHrqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPhol6zGHy2r"
      },
      "source": [
        "## 3. Load data\n",
        "Load test dataset for rider3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip test files\n",
        "!unzip /content/drive/MyDrive/opencampus_all_files/rider3/Rider3_test.zip"
      ],
      "metadata": {
        "id": "fL0bQaaeeNGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaler configuration\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/opencampus_all_files/rider3/combined_data_r3.csv')\n",
        "\n",
        "# Feature and target variable selection\n",
        "Time_real = df['Time']\n",
        "X = df[['Elevation', 'Slope_prev', 'Slope_next',  'Angle', 'Distance', 'Cumulative_Slope']] # Cumulative slope\n",
        "y = df['Speed']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Normalize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "q-y-b84detu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1. Rider 1"
      ],
      "metadata": {
        "id": "dI4qCGMNeaAg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvmAsNqmHy2r"
      },
      "outputs": [],
      "source": [
        "# Define the folder containing the files\n",
        "input_folder_path = '/content/content/Rider3_test'\n",
        "output_folder_path = '/content/content/Rider3_test_r1/'\n",
        "\n",
        "# Create output_folder_path\n",
        "if not os.path.exists(output_folder_path):\n",
        "    os.makedirs(output_folder_path)\n",
        "\n",
        "# List all CSV files in the folder\n",
        "csv_files = [f for f in os.listdir(input_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Iterate through each file in the folder\n",
        "for file in csv_files:\n",
        "    file_path = os.path.join(input_folder_path, file)\n",
        "\n",
        "    # Read test file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Feature selection\n",
        "    real_time = df['Time']\n",
        "    X = df[['Elevation', 'Slope_prev', 'Slope_next', 'Angle', 'Distance', 'Cumulative_Slope']]\n",
        "    y = df['Speed']\n",
        "\n",
        "    # Make predictions on the new data\n",
        "    X_new_scaled = scaler.transform(X)\n",
        "    y_pred = r1_model.predict(X_new_scaled)\n",
        "\n",
        "    # Add the predicted values as a new column 'Speed_pred' in the original DataFrame\n",
        "    df['Speed_pred'] = y_pred\n",
        "\n",
        "    # Calculate MAE between df['Speed_pred'] and df['Speed']\n",
        "    mae = mean_absolute_error(df['Speed'], df['Speed_pred'])\n",
        "    print(f\"File: {file} | Mean Absolute Error: {mae}\")\n",
        "\n",
        "    # Initialize Time column\n",
        "    df['Time_pred'] = float(df['Time'].iloc[0])\n",
        "\n",
        "    # Compute predicted time\n",
        "    for i in range(2, len(df)):\n",
        "        if df.loc[i, 'Speed_pred'] < 0:\n",
        "            df.loc[i, 'Speed_pred'] = 0\n",
        "        if pd.notna(df.loc[i, 'Speed_pred']):\n",
        "            if df.loc[i, 'Speed_pred'] != 0:\n",
        "                df.loc[i, 'Time_pred'] = (\n",
        "                    df.loc[i - 1, 'Time_pred'] +\n",
        "                    (df.loc[i, 'Distance'] - df.loc[i - 1, 'Distance']) / df.loc[i, 'Speed_pred']\n",
        "                )\n",
        "            else:\n",
        "                df.loc[i, 'Time_pred'] = df.loc[i - 1, 'Time_pred']\n",
        "\n",
        "    # Save the processed DataFrame to a new file (optional)\n",
        "    output_file_path = os.path.join(output_folder_path, f\"nn_{file}\")\n",
        "    df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2. Rider 3"
      ],
      "metadata": {
        "id": "NrQc30CffGlN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIh-wNwBfGlP"
      },
      "outputs": [],
      "source": [
        "# Define the folder containing the files\n",
        "input_folder_path = '/content/content/Rider3_test'\n",
        "output_folder_path = '/content/content/Rider3_test_r3/'\n",
        "\n",
        "# Create output_folder_path\n",
        "if not os.path.exists(output_folder_path):\n",
        "    os.makedirs(output_folder_path)\n",
        "\n",
        "# List all CSV files in the folder\n",
        "csv_files = [f for f in os.listdir(input_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Iterate through each file in the folder\n",
        "for file in csv_files:\n",
        "    file_path = os.path.join(input_folder_path, file)\n",
        "\n",
        "    # Read test file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Feature selection\n",
        "    real_time = df['Time']\n",
        "    X = df[['Elevation', 'Slope_prev', 'Slope_next', 'Angle', 'Distance', 'Cumulative_Slope']]\n",
        "    y = df['Speed']\n",
        "\n",
        "    # Make predictions on the new data\n",
        "    X_new_scaled = scaler.transform(X)\n",
        "    y_pred = r3_model.predict(X_new_scaled)\n",
        "\n",
        "    # Add the predicted values as a new column 'Speed_pred' in the original DataFrame\n",
        "    df['Speed_pred'] = y_pred\n",
        "\n",
        "    # Calculate MAE between df['Speed_pred'] and df['Speed']\n",
        "    mae = mean_absolute_error(df['Speed'], df['Speed_pred'])\n",
        "    print(f\"File: {file} | Mean Absolute Error: {mae}\")\n",
        "\n",
        "    # Initialize Time column\n",
        "    df['Time_pred'] = float(df['Time'].iloc[0])\n",
        "\n",
        "    # Compute predicted time\n",
        "    for i in range(2, len(df)):\n",
        "        if df.loc[i, 'Speed_pred'] < 0:\n",
        "            df.loc[i, 'Speed_pred'] = 0\n",
        "        if pd.notna(df.loc[i, 'Speed_pred']):\n",
        "            if df.loc[i, 'Speed_pred'] != 0:\n",
        "                df.loc[i, 'Time_pred'] = (\n",
        "                    df.loc[i - 1, 'Time_pred'] +\n",
        "                    (df.loc[i, 'Distance'] - df.loc[i - 1, 'Distance']) / df.loc[i, 'Speed_pred']\n",
        "                )\n",
        "            else:\n",
        "                df.loc[i, 'Time_pred'] = df.loc[i - 1, 'Time_pred']\n",
        "\n",
        "    # Save the processed DataFrame to a new file (optional)\n",
        "    output_file_path = os.path.join(output_folder_path, f\"nn_{file}\")\n",
        "    df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3. Rider 7"
      ],
      "metadata": {
        "id": "NPGkcWVSfUyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY_KXkTkfUyE"
      },
      "outputs": [],
      "source": [
        "# Define the folder containing the files\n",
        "input_folder_path = '/content/content/Rider3_test'\n",
        "output_folder_path = '/content/content/Rider3_test_r7/'\n",
        "\n",
        "# Create output_folder_path\n",
        "if not os.path.exists(output_folder_path):\n",
        "    os.makedirs(output_folder_path)\n",
        "\n",
        "# List all CSV files in the folder\n",
        "csv_files = [f for f in os.listdir(input_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Iterate through each file in the folder\n",
        "for file in csv_files:\n",
        "    file_path = os.path.join(input_folder_path, file)\n",
        "\n",
        "    # Read test file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Feature selection\n",
        "    real_time = df['Time']\n",
        "    X = df[['Elevation', 'Slope_prev', 'Slope_next', 'Angle', 'Distance', 'Cumulative_Slope']]\n",
        "    y = df['Speed']\n",
        "\n",
        "    # Make predictions on the new data\n",
        "    X_new_scaled = scaler.transform(X)\n",
        "    y_pred = r7_model.predict(X_new_scaled)\n",
        "\n",
        "    # Add the predicted values as a new column 'Speed_pred' in the original DataFrame\n",
        "    df['Speed_pred'] = y_pred\n",
        "\n",
        "    # Calculate MAE between df['Speed_pred'] and df['Speed']\n",
        "    mae = mean_absolute_error(df['Speed'], df['Speed_pred'])\n",
        "    print(f\"File: {file} | Mean Absolute Error: {mae}\")\n",
        "\n",
        "    # Initialize Time column\n",
        "    df['Time_pred'] = float(df['Time'].iloc[0])\n",
        "\n",
        "    # Compute predicted time\n",
        "    for i in range(2, len(df)):\n",
        "        if df.loc[i, 'Speed_pred'] < 0:\n",
        "            df.loc[i, 'Speed_pred'] = 0\n",
        "        if pd.notna(df.loc[i, 'Speed_pred']):\n",
        "            if df.loc[i, 'Speed_pred'] != 0:\n",
        "                df.loc[i, 'Time_pred'] = (\n",
        "                    df.loc[i - 1, 'Time_pred'] +\n",
        "                    (df.loc[i, 'Distance'] - df.loc[i - 1, 'Distance']) / df.loc[i, 'Speed_pred']\n",
        "                )\n",
        "            else:\n",
        "                df.loc[i, 'Time_pred'] = df.loc[i - 1, 'Time_pred']\n",
        "\n",
        "    # Save the processed DataFrame to a new file (optional)\n",
        "    output_file_path = os.path.join(output_folder_path, f\"nn_{file}\")\n",
        "    df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Results\n",
        "Compare results of different models on the same files."
      ],
      "metadata": {
        "id": "2bOhLPztOTOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1. Rider 1"
      ],
      "metadata": {
        "id": "PPypgeBegDbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for processed files\n",
        "output_folder_path = '/content/content/Rider3_test_r1/'\n",
        "summary_r1 = []\n",
        "processed_files = [f for f in os.listdir(output_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "for file in processed_files:\n",
        "    file_path = os.path.join(output_folder_path, file)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract the last value of 'Time' and 'Time_pred'\n",
        "    last_time = df['Time'].iloc[-1]\n",
        "    last_time_pred = df['Time_pred'].iloc[-1]\n",
        "\n",
        "    # Calculate percentage difference\n",
        "    percentage_diff = ((last_time_pred - last_time) / last_time) * 100 if last_time != 0 else None\n",
        "\n",
        "    # Append to summary\n",
        "    summary_r1.append({\n",
        "        'file': file,\n",
        "        'last_time': last_time,\n",
        "        'last_time_pred': last_time_pred,\n",
        "        'percentage_diff': percentage_diff\n",
        "    })"
      ],
      "metadata": {
        "id": "_UwHqpD_f0aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2. Rider 3"
      ],
      "metadata": {
        "id": "TNf5pNNAgFw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for processed files\n",
        "output_folder_path = '/content/content/Rider3_test_r3/'\n",
        "summary_r3 = []\n",
        "processed_files = [f for f in os.listdir(output_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "for file in processed_files:\n",
        "    file_path = os.path.join(output_folder_path, file)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract the last value of 'Time' and 'Time_pred'\n",
        "    last_time = df['Time'].iloc[-1]\n",
        "    last_time_pred = df['Time_pred'].iloc[-1]\n",
        "\n",
        "    # Calculate percentage difference\n",
        "    percentage_diff = ((last_time_pred - last_time) / last_time) * 100 if last_time != 0 else None\n",
        "\n",
        "    # Append to summary\n",
        "    summary_r3.append({\n",
        "        'file': file,\n",
        "        'last_time': last_time,\n",
        "        'last_time_pred': last_time_pred,\n",
        "        'percentage_diff': percentage_diff\n",
        "    })"
      ],
      "metadata": {
        "id": "hoRMgwfRgFw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3. Rider 7"
      ],
      "metadata": {
        "id": "tmCkm16_gGNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for processed files\n",
        "output_folder_path = '/content/content/Rider3_test_r7/'\n",
        "summary_r7 = []\n",
        "processed_files = [f for f in os.listdir(output_folder_path) if f.endswith('.csv')]\n",
        "\n",
        "for file in processed_files:\n",
        "    file_path = os.path.join(output_folder_path, file)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract the last value of 'Time' and 'Time_pred'\n",
        "    last_time = df['Time'].iloc[-1]\n",
        "    last_time_pred = df['Time_pred'].iloc[-1]\n",
        "\n",
        "    # Calculate percentage difference\n",
        "    percentage_diff = ((last_time_pred - last_time) / last_time) * 100 if last_time != 0 else None\n",
        "\n",
        "    # Append to summary\n",
        "    summary_r7.append({\n",
        "        'file': file,\n",
        "        'last_time': last_time,\n",
        "        'last_time_pred': last_time_pred,\n",
        "        'percentage_diff': percentage_diff\n",
        "    })"
      ],
      "metadata": {
        "id": "8GXIuYNqgGNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.4. Summary"
      ],
      "metadata": {
        "id": "5bj1jme5gQFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print summary\n",
        "summary_r1_df = pd.DataFrame(summary_r1).sort_values(by=['file'])\n",
        "summary_r3_df = pd.DataFrame(summary_r3).sort_values(by=['file'])\n",
        "summary_r7_df = pd.DataFrame(summary_r7).sort_values(by=['file'])\n",
        "\n",
        "# Selecting and renaming columns from each dataframe\n",
        "r1_selected = summary_r1_df[['file', 'last_time', 'last_time_pred']].rename(columns={\n",
        "    'last_time': 'real_r3_time',\n",
        "    'last_time_pred': 'pred_r1_time'\n",
        "})\n",
        "\n",
        "r3_selected = summary_r3_df[['last_time_pred']].rename(columns={\n",
        "    'last_time_pred': 'pred_r3_time'\n",
        "})\n",
        "\n",
        "r7_selected = summary_r7_df[['last_time_pred']].rename(columns={\n",
        "    'last_time_pred': 'pred_r7_time'\n",
        "})\n",
        "\n",
        "# Combining into one table\n",
        "combined_df = r1_selected.copy()\n",
        "combined_df['pred_r3_time'] = r3_selected['pred_r3_time']\n",
        "combined_df['pred_r7_time'] = r7_selected['pred_r7_time']\n",
        "\n",
        "# Display or use the combined DataFrame\n",
        "print(combined_df)"
      ],
      "metadata": {
        "id": "YBMRuYnCgTDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}